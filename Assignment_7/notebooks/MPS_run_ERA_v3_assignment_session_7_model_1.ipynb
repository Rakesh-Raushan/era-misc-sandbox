{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gMXa4ypzgv8P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEED setup for Reproducibility and deciding device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use MPS?: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "# Apple metal or Nvidia CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.mps.is_available()\n",
    "\n",
    "# seed for repeatablility:\n",
    "\n",
    "# for all devices\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# for specific acc\n",
    "if use_cuda:\n",
    "    print(f\"Use CUDA?:{use_cuda}\")\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif use_mps:\n",
    "    print(f\"Use MPS?: {use_mps}\")\n",
    "    torch.mps.manual_seed(SEED)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms, datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase transformations\n",
    "train_transforms = transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,)), # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
    "                                       ])\n",
    "\n",
    "# Test Phase transformations\n",
    "test_transforms = transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ])\n",
    "\n",
    "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
    "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "use_cuda = False\n",
    "use_mps = False\n",
    "\n",
    "dataloader_args = dict(shuffle=True,\n",
    "                       batch_size=128,\n",
    "                       num_workers=0,\n",
    "                       pin_memory=True) \\\n",
    "                                        if use_cuda or use_mps else \\\n",
    "                    dict(shuffle=True,\n",
    "                         batch_size=128)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           **dataloader_args)\n",
    "test_loader = torch.utils.data.DataLoader(test,\n",
    "                                          **dataloader_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 1\n",
    "\n",
    "```Target```:\n",
    "- Get the set-up right for basic NN training\n",
    "    - Set Transforms\n",
    "    - Set Data Loader\n",
    "    - Set Basic Working Code\n",
    "    - Set Basic Training  & Test Loop\n",
    "\n",
    "  AND\n",
    "\n",
    "- Reduce parmas < 8k\n",
    "      - Use GAP instead of FCN\n",
    "- Use batchnorm for efficiency\n",
    "\n",
    "\n",
    "```Results```:\n",
    "- Parameters: 6922\n",
    "- Best Training Accuracy: 98.83\n",
    "- Best Test Accuracy: 98.67\n",
    "\n",
    "```Analysis```: Model is overfitting and needs regularisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyNetwork1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #INPUT BLOCK\n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3), #inch=1, outch=8, size=26, rf=3, j=1\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        #CONV BLOCK 1\n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv2d(8,8, 3), #inch=8, outch=8, size=24, rf=5, j=1\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv2d(8,8, 3), #inch=8, inout=8, size=22, rf=7, j=1\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convblock4 = nn.Sequential(\n",
    "            nn.Conv2d(8,16, 3), #inch=8, inout=16, size=20, rf=9, j=1\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # TRANSITION BLOCK\n",
    "        self.convblock5 = nn.Sequential(\n",
    "            nn.Conv2d(16,8,3), #inch32, outch=8, size=18, rf=11, j=1\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        #CONV BLOCK 2\n",
    "        self.convblock6 = nn.Sequential(\n",
    "            nn.Conv2d(8,8,3), #inch=8, outch=8, size=16, rf=13, j=1\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convblock7 = nn.Sequential(\n",
    "            nn.Conv2d(8,16,3), #inch=8, outch=16, size=12, rf=17, j=1\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # OUTPUT BLOCK\n",
    "        self.convblock8 = nn.Sequential(\n",
    "            nn.Conv2d(16,10,3), #inch=16, outch=10, size=10, rf=19, j=1\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "        #LINEAR LAYER\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BLOCK 1\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        # TRANSITION\n",
    "        x = self.convblock5(x)\n",
    "        # BLOCK 2\n",
    "        x = self.convblock6(x)\n",
    "        x = self.convblock7(x)\n",
    "        # OUTPUT\n",
    "        x = self.convblock8(x)\n",
    "        # GAP\n",
    "        x = self.gap(x)\n",
    "        # here the shape of x will be (batch_size,10,14,14), before passing let's reshape it to (batch_size, 10*14*14)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nyvn5pvDqdS4",
    "outputId": "250335c3-e9ff-4976-ed65-3ab918b07ed3"
   },
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "model_1 = MyNetwork1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 26, 26]              80\n",
      "       BatchNorm2d-2            [-1, 8, 26, 26]              16\n",
      "              ReLU-3            [-1, 8, 26, 26]               0\n",
      "            Conv2d-4            [-1, 8, 24, 24]             584\n",
      "       BatchNorm2d-5            [-1, 8, 24, 24]              16\n",
      "              ReLU-6            [-1, 8, 24, 24]               0\n",
      "            Conv2d-7            [-1, 8, 22, 22]             584\n",
      "       BatchNorm2d-8            [-1, 8, 22, 22]              16\n",
      "              ReLU-9            [-1, 8, 22, 22]               0\n",
      "           Conv2d-10           [-1, 16, 20, 20]           1,168\n",
      "      BatchNorm2d-11           [-1, 16, 20, 20]              32\n",
      "             ReLU-12           [-1, 16, 20, 20]               0\n",
      "           Conv2d-13            [-1, 8, 18, 18]           1,160\n",
      "      BatchNorm2d-14            [-1, 8, 18, 18]              16\n",
      "             ReLU-15            [-1, 8, 18, 18]               0\n",
      "           Conv2d-16            [-1, 8, 16, 16]             584\n",
      "      BatchNorm2d-17            [-1, 8, 16, 16]              16\n",
      "             ReLU-18            [-1, 8, 16, 16]               0\n",
      "           Conv2d-19           [-1, 16, 14, 14]           1,168\n",
      "      BatchNorm2d-20           [-1, 16, 14, 14]              32\n",
      "             ReLU-21           [-1, 16, 14, 14]               0\n",
      "           Conv2d-22           [-1, 10, 12, 12]           1,450\n",
      "AdaptiveAvgPool2d-23             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 6,922\n",
      "Trainable params: 6,922\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.65\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model_1, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# define metrics to calc\n",
    "\n",
    "train_losses = []\n",
    "train_acc = []\n",
    "\n",
    "test_losses = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # tqdm iterator\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    # correct and processed vars\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "\n",
    "    # loop on batches of data\n",
    "    for batch_idx, (data,target) in enumerate(pbar):\n",
    "        #send data, targte to training device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Initialize grad to zero for the fresh batch grad accumuation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pred with model\n",
    "        y_pred = model(data)\n",
    "\n",
    "        # calc loss\n",
    "        batch_loss = F.nll_loss(y_pred, target)\n",
    "        train_losses.append(batch_loss)\n",
    "\n",
    "        # backprop loss to calc and acc grad w.r.t loss of batch\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # update weights as per losses seen in this batch\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate correct pred count and acc for batch\n",
    "        pred_labels = y_pred.argmax(dim=1, keepdim=True)\n",
    "        correct_count_batch = pred_labels.eq(target.view_as(pred_labels)).sum().item()\n",
    "\n",
    "        # update total correct and total processed so far\n",
    "        correct+= correct_count_batch\n",
    "        processed+= len(data)\n",
    "\n",
    "        # set pbar desc\n",
    "        pbar.set_description(desc=f'batch Loss = {batch_loss.item()} batch_id = {batch_idx} accuracy = {100*correct/processed:.02f}'\n",
    "                            )\n",
    "        #append train acc\n",
    "        train_acc.append(100*correct/processed)\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # define var to calc correct and processed\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    test_loss = 0 # seeing loss as the code runs has no value for test\n",
    "\n",
    "    # set a no grad context\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            #send data, target to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "    \n",
    "            # do pred\n",
    "            y_pred = model(data)\n",
    "    \n",
    "            #calc loss for batch as summed and update total test loss\n",
    "            batch_loss = F.nll_loss(y_pred, target, reduction='sum').item()\n",
    "            test_loss+= batch_loss\n",
    "            # collect loss\n",
    "            test_losses.append(batch_loss)\n",
    "    \n",
    "            # count correct\n",
    "            pred_labels = y_pred.argmax(dim=1, keepdim=True)\n",
    "            correct_batch = pred_labels.eq(target.view_as(pred_labels)).sum().item()\n",
    "    \n",
    "            #update correct\n",
    "            correct+= correct_batch\n",
    "            processed+= len(data)\n",
    "\n",
    "    # avg loss on test makes more sense to avg it\n",
    "    test_loss/= processed\n",
    "    # collect avg losses\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(f'\\n Test set avg loss: {test_loss:.4f} \\\n",
    "                Accuracy: {correct}/{processed}, {100*correct/processed:.2f}'\n",
    "         )\n",
    "\n",
    "    test_acc.append(100*correct/processed)\n",
    "\n",
    "    return test_loss, round(100*correct/processed, 1)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for n Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.1570795327425003 batch_id = 468 accuracy = 82.58: 100%|██| 469/469 [00:09<00:00, 51.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.1699                 Accuracy: 9565/10000, 95.65\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.1131078377366066 batch_id = 468 accuracy = 96.06: 100%|██| 469/469 [00:08<00:00, 55.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.1111                 Accuracy: 9677/10000, 96.77\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.12369922548532486 batch_id = 468 accuracy = 97.24: 100%|█| 469/469 [00:09<00:00, 51.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0889                 Accuracy: 9741/10000, 97.41\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.09919320791959763 batch_id = 468 accuracy = 97.73: 100%|█| 469/469 [00:08<00:00, 54.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0896                 Accuracy: 9715/10000, 97.15\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.06370348483324051 batch_id = 468 accuracy = 97.94: 100%|█| 469/469 [00:08<00:00, 54.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0761                 Accuracy: 9781/10000, 97.81\n",
      "EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.06324979662895203 batch_id = 468 accuracy = 98.22: 100%|█| 469/469 [00:08<00:00, 56.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0574                 Accuracy: 9818/10000, 98.18\n",
      "EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.02165728621184826 batch_id = 468 accuracy = 98.33: 100%|█| 469/469 [00:08<00:00, 56.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0479                 Accuracy: 9857/10000, 98.57\n",
      "EPOCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.0949222519993782 batch_id = 468 accuracy = 98.40: 100%|██| 469/469 [00:08<00:00, 55.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0604                 Accuracy: 9815/10000, 98.15\n",
      "EPOCH: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.03512514755129814 batch_id = 468 accuracy = 98.55: 100%|█| 469/469 [00:08<00:00, 52.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0487                 Accuracy: 9840/10000, 98.40\n",
      "EPOCH: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.09983039647340775 batch_id = 468 accuracy = 98.54: 100%|█| 469/469 [00:09<00:00, 50.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0404                 Accuracy: 9870/10000, 98.70\n",
      "EPOCH: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.02062891609966755 batch_id = 468 accuracy = 98.69: 100%|█| 469/469 [00:09<00:00, 50.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0393                 Accuracy: 9877/10000, 98.77\n",
      "EPOCH: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.020275773480534554 batch_id = 468 accuracy = 98.73: 100%|█| 469/469 [00:09<00:00, 50.49it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0394                 Accuracy: 9879/10000, 98.79\n",
      "EPOCH: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.11575008183717728 batch_id = 468 accuracy = 98.83: 100%|█| 469/469 [00:11<00:00, 41.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0459                 Accuracy: 9863/10000, 98.63\n",
      "EPOCH: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atch Loss = 0.05142282322049141 batch_id = 468 accuracy = 98.83: 100%|█| 469/469 [00:11<00:00, 40.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set avg loss: 0.0448                 Accuracy: 9867/10000, 98.67\n"
     ]
    }
   ],
   "source": [
    "# train without scheduler\n",
    "\n",
    "# initialize model on device\n",
    "model = model_1.to(device)\n",
    "\n",
    "# initialize optimizer with model params and lr\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Set total epochs\n",
    "EPOCHS = 14\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    train(model, device, train_loader, optimizer)\n",
    "    test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
